---
title             : "Common or Distinct Attention Mechanisms for Contrast and Assimilation?"
shorttitle        : "Contrast and Assimilation"

author: 
  - name          : "Hope K. Snyder"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "205 McAlester Hall, University of Missouri, Columbia, MO 65211"
    email         : "hks7w2@mail.missouri.edu"
  - name          : "Sean M. Rafferty"
    affiliation   : "1"
  - name          : "Julia M. Haaf"
    affiliation   : "1"
  - name          : "Jeffery N. Rouder"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Missouri"

author_note: >
  This paper was written in R-Markdown with code for data analysis integrated into the text.  The Markdown script is open and freely available at https://github.com/PerceptionAndCognitionLab/ctx-flanker/tree/public/papers/current.  The data were *born open* [@Rouder:2016] and are freely available at https://github.com/PerceptionCognitionLab/data1/tree/master/ctxIndDif/flankerMorph4

abstract: >
  The ability to inhibit distracting elements in the environment while focusing on specific targets is crucial.  In most tasks, like Stroop or priming, the to-be-ignored distractors affect the response in a certain direction that we call assimilation.  The response is more like the distractors than otherwise.  Yet, in some tasks, contrast tasks, the opposite holds.  We ask here if individuals with strong inhibition in assimilation-type tasks also have strong inhibition in contrast tasks.  Assimilation and contrast are often thought to have different underlying psychological mechanisms, and a correlational analysis serves as a test of this hypothesis vs. a more unified account of inhibition.  To answer the question of whether inhibition across contrastive and assimilative contexts is common or distinct, we assess the correlations with a hierarchical Bayesian model.  We designed tasks with common stimuli that had large assimilation or contrast effects depending only on the surrounding context.  Critically, a positive correlation was found---individuals who better inhibited a contrast-inducing contexts also better inhibited assimilation-inducing contexts.  These results indicate inhibition is seemingly common, at least in part, across contrast and assimilation tasks.
  
keywords          : "Inhibition, Selective Attention, Contrast Effects, Assimilation Effects"
wordcount         : " "

bibliography      : ["r-references.bib","bibfile.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

```{r start, include = FALSE}
library("papaja")
```

The concepts of *spatial selective attention* and *response inhibition* have been topical in the psychological literature since at least @Helmholtz:1867 and @James:1890a.  The ability to inhibit some elements in the environment while focusing on other target elements is crucial to proper functioning [e.g., @Broadbent:1958;@Cowan:1995].  Failures to properly inhibit responses to inappropriate elements in the environment are used to describe and understand pathological development [@Barkley:1997;@Hasher:Zacks:1988].  Inhibition is usually treated as a fairly unified or homogenous concept.  It may be measured in tasks where the goal is to select or identify target items in the presence of to-be-ignored distractors.

In many spatial interference tasks, such as Eriksen interference [@Eriksen:Eriksen:1974], the effect is usually in a specific direction that we term *assimilation*.  Assimilation is displayed in Figure \@ref(fig:past)A where the goal is to identify the center letter as either an "A" or an "H."  People are more likely to misidentify the target *H* as an "A" when surrounded by *A*'s than surrounded by *H*'s.
Because they are making responses seemingly driven by the identity of the flankers rather than the target, we say their inihibition failure leads to an *assimilation* of the background. 

Assimilation, however, is not the only outcome.  @Rouder:King:2003 used a modified version of the flanker task and found the opposite effects, which are called here *contrast* effects.  Rather than use well-formed letters, their targets were morphed letters between *A* and *H* (see Figure \@ref(fig:past)B).  Perhaps suprisingly, morphs surrounded by *H*'s were *less likely* to be identified as "H" than morphs surrounded by *A*'s.   This effect is exactly opposite because inhibition failures lead to a response least like the background.

The main question we ask is whether spatial selective attention is mediated by common or distinct mechanisms across tasks that typically lead to the opposite patterns of contrast and assimilation.  We address this question by studying the correlation of inhibition abilities across people.  Are people who are good at ignoring the surrounding information in a standard flanker task that promote assimilation also good at ignoring the surrounding information in a modified flanker task that promotes contrast?

There are some studies that support a common mechanism while there are others that support the distinct mechanisms.  Support for the common-mechanism view is as follows:  Most proposed behavioral and neural mechanisms of selective attention rely on a narrowing of receptive fields.  This narrowing process occurs  whether the to-be-excluded information is constrastive or assimilative [e.g., @Cowan:1995;@Desimone:Duncan:1995;@Eriksen:Schultz:1979;@Hedden:Gabrieli:2010].  Behavioral evidence comes from @Miyake:etal:2000 who studied individual differences.  When variability across a range of executive-function tasks is decomposed, one recovered latent factor is a unified concept of inhibition.  

Yet, there are strong arguments that contrast and assimilation are not the same process.  Most theories of contrast rely on a center-surround organization of low-level, perceptual receptive field structures [@Palmer:1999]. Most theories of assimilation flanker effects, however, are failures of response inhibition, which is conceptualized as a higher-level, top down process [@Eriksen:Eriksen:1974].  Behavioral evidence for distinct processes comes from @Rouder:King:2003 whose design is a bit complicated:.  These researchers associated the letters *c* and *A* with one response, denoted $R_1$, and *e* and *H* with another, denoted $R_2$.  When well-formed letters were the target, assimilation to the response assignment of the background was observed.  For well-formed *A* letters, backgrounds of *c* resulted in higher proportion of $R_1$ response than did backgrounds of *e*.  When morphed letters were targets, the effect varied.  If morphs between *A* and *H* were sourounded by either *A* or *H*, the effect was contrastive.  Yet, if these same morphs were surorunded by *c* or *e*, the effect was assimilative.  @Rouder:King:2003 interpreted the assimilative effect to be at the response level and the constrative effect to be at a perceptual level.

We take a correlational approach with better stimulus control than @Rouder:King:2003.  We sought targets and backgrounds such that whether there was contrast or assimilation was a function of the background rather than the target.  Our targets were morphed letters like those in Figure \@ref(fig:past)B.  Our backgrounds came in two types, either a letter context such as in Figure \@ref(fig:past)B or a word context such as in \@ref(fig:past)C.  For the letter context, we expect large constrast effects as was observed by @Rouder:King:2003.  The rationale for the word context comes from @Neisser:1967.  We expect that a morph between *A* and *H* will be judged more "A" like in *C_T* context than *T_E* context.  This effect is assimilative, and it has been repeatedly demonstrated that there is an assimilative effect of word contexts for both visually and orally presented letters [@Baron:Thurston:1973;@Reicher:1969].  As an aside, this assimilation effect has been a motivating phenomenon in the development of connectionist models [e.g., @Carpenter:Grossberg:1987;@Rumelhart:McClelland:1982] where word nodes feed positive activation to corresponding letter nodes.

In the following experiment, each participant identified several *A*-to-*H* morphs (see the top row of Figure \@ref(fig:stimulus)).  These target morphs were embedded within four background contexts (see the bottom row of Figure \@ref(fig:stimulus)).  By comparing performance in the *A*-letter and *H*-letter contexts, we assess each participant's ability to inhibit contrastive information.  By comparing performance in the *C_T*-word and *T_E*-word contexts, we assess each participant's ability to inhibit assimilative information.  Note that for each target, there are assimilative and contrastive background contexts such that the critical comparisons may be made across backgrounds.  To asses the question of whether inhibition across contrastive and assimilative contexts is common or distinctt, we assess the correlation across individuals of their inhibition within the two context types.

```{r past, fig.width= 4,fig.height=3,fig.cap="Three flanker paradigms.  The participants' task is to identify the center letter as either an A or an H. **A.** Conventional Eriksen & Eriksen (1974) flanker paradigm results in assimilation.  **B.** Modified paradigm with morph-letter targets results in contrast (Rouder & King, 2003). **C.** Word contexts result in assimilation (Neisser, 1967).",cache=TRUE}
### general variables
sep=.1
dist=1+sep
x=c(0,0,0,1,1,1,2,2,2)*dist
y=c(0,1,2,0,1,2,0,1,2)*dist

myLet=function(a,pos){
  b=(-1/9)*a+(1/18)
  x1=x[pos]+c(0+b,1-b,(a/2)+(b))
  x2=x[pos]+c(a+b,1-a-b,1-(a/2)-b)
  y1=y[pos]+c(0,0,.5)
  y2=y[pos]+c(1,1,.5)
  segments(x1,y1,x2,y2,lwd=3)
}

myC=function(pos){
  z0=.2
  z1=.78
  x1=x[pos]+c(z1,z0,z0)
  x2=x[pos]+c(z0,z0,z1)
  y1=y[pos]+c(0,0,1)
  y2=y[pos]+c(0,1,1)
  segments(x1,y1,x2,y2,lwd=3)
}

myT=function(pos){
  z0=0
  z1=1
  x1=x[pos]+c(.5,z0)
  x2=x[pos]+c(.5,z1)
  y1=y[pos]+c(z0,z1)
  y2=y[pos]+c(z1,z1)
  segments(x1,y1,x2,y2,lwd=3)
}

myE=function(pos){
  z0=.1
  z1=.9
  x1=x[pos]+c(z1,z0,z0,z0)
  x2=x[pos]+c(z0,z0,z1,z0+.67*(z1-z0))
  y1=y[pos]+c(0,0,1,.5)
  y2=y[pos]+c(0,1,1,.5)
  segments(x1,y1,x2,y2,lwd=3)
}

cat=function(center){
  par(mar=c(0,3.5,0,1.5))
  plot(c(0,3*dist),c(-.25,dist+.25),axes=F,typ='n',ylab=" ")
  myC(1)
  myLet(center/2,4)
  myT(7)
}

the=function(center){
  par(mar=c(0,1.5,0,3.5))
  plot(c(0,3*dist),c(-.25,dist+.25),axes=F,typ='n')
  myT(1)
  myLet(center/2,4)
  myE(7)
}

myArray=function(center,surround){
  par(mar=c(0,1.5,0,3.5))
  plot(c(0,3*dist),c(-.25,dist+.25),axes=F,typ='n')
  myLet(surround/2,1)
  myLet(center/2,4)
  myLet(surround/2,7)
}

myArray2=function(center,surround){
  par(mar=c(0,3.5,0,1.5))
  plot(c(0,3*dist),c(-.25,dist+.25),axes=F,typ='n',ylab=" ")
  myLet(surround/2,1)
  myLet(center/2,4)
  myLet(surround/2,7)
}

par(mfrow=c(3,2))
############
# Plot 1
############
myArray2(0,0)
mtext("A.", line = 1, padj = 0, side = 2, las = 1, cex = 1.5)
mtext("vs.", line = 1, padj = 0, side = 4, las = 1)
myArray(0,1)

############
# Plot 2
############
myArray2((12/20),0)
mtext("B.", line = 1, padj = 0, side = 2, las = 1, cex = 1.5)
mtext("vs.", line = 1, padj = 0, side = 4, las = 1)
myArray((12/20),1)

############
# Plot 3
############
cat((12/20))
mtext("C.", line = 1, padj = 0, side = 2, las = 1, cex = 1.5)
mtext("vs.", line = 1, padj = 0, side = 4, las = 1)
the((12/20))
```

# Method

## Participants
Ninety-nine undergraduates from the University of Missouri served as participants as part of an introductory course requirement.  Data from three participants were discarded due to a computer error and data from an additional three were discarded because twenty or more of their responses were shorter than a criterial 200 ms in duration.  The data from the remaining $93$ participants were used in analysis.  We analyzed the data with Bayesian methods, to be discussed subsequently, in a sequential fashion stopping when either the data were clearly evidential of the cited patterns [@Rouder:2014] or the semester ended.  Both occured together.  

## Design
The experiment was a $5\times 2 \times 2$ within-subject factorial design.  The first factor was the target, and it was manipulated through 5 levels from the letter *H* through the morphs to the letter *A*.  The second factor was the context type, and the background was either a letter or a word.  The final variable was context direction, a context that promotes  "A" or "H" responses.  We coded the *H* background and the *C_T* background as promoting "A" responses based on prior literature.  This coding does not determine the direction of results; it simply provides a clear language for disucssing them.

## Material
The stimuli are shown in Figure \@ref(fig:stimulus).   All targets appear in each of the four contexts, though the rates are not equal.  To emphasize the morphs, the three central targets in Figure \@ref(fig:stimulus) were each twice as likely to appear than each well-formed letter.

```{r stimulus, fig.width= 6,fig.height=3.5,fig.cap="Stimuli.  **Top** Five targets.  **Bottom**  Four contexts.  The stimulus on a trial consisted of one of the four targets placed into the bank center location of the background context.", cache=TRUE}
layout(matrix(c(0,1,1,0,2,3,4,5,2,3,4,5),nrow=3,byrow=T))

### general variables
background=c(1:4,6:9)
wbackground=c(1,3,4,6,7,9)
sep=.1
dist=1+sep
x=c(0,0,0,1,1,1,2,2,2,3,3,3,4,4,4)*dist
y=c(0,1,2,0,1,2,0,1,2,0,1,2,0,1,2)*dist

### FUNCTIONS USED
myLet=function(a,pos){
  b=(-1/9)*a+(1/18)
  x1=x[pos]+c(0+b,1-b,(a/2)+(b))
  x2=x[pos]+c(a+b,1-a-b,1-(a/2)-b)
  y1=y[pos]+c(0,0,.5)
  y2=y[pos]+c(1,1,.5)
  segments(x1,y1,x2,y2,lwd=3)
}

myBox=function(pos){
  z0=0
  z1=1		
  x1=x[pos]+c(z0,z0,z1,z1,z0,z1)
  x2=x[pos]+c(z0,z1,z1,z0,z1,z0)
  y1=y[pos]+c(0,1,1,0,0,0)
  y2=y[pos]+c(1,1,0,0,1,1)
  segments(x1,y1,x2,y2,lwd=3)
}

myC=function(pos){
  z0=.2
  z1=.78
  x1=x[pos]+c(z1,z0,z0)
  x2=x[pos]+c(z0,z0,z1)
  y1=y[pos]+c(0,0,1)
  y2=y[pos]+c(0,1,1)
  segments(x1,y1,x2,y2,lwd=3)
}

myT=function(pos){
  z0=0
  z1=1
  x1=x[pos]+c(.5,z0)
  x2=x[pos]+c(.5,z1)
  y1=y[pos]+c(z0,z1)
  y2=y[pos]+c(z1,z1)
  segments(x1,y1,x2,y2,lwd=3)
}

myE=function(pos){
  z0=.1
  z1=.9
  x1=x[pos]+c(z1,z0,z0,z0)
  x2=x[pos]+c(z0,z0,z1,z0+.67*(z1-z0))
  y1=y[pos]+c(0,0,1,.5)
  y2=y[pos]+c(0,1,1,.5)
  segments(x1,y1,x2,y2,lwd=3)
}

cat=function(){
  par(mar=c(0,0,0,0))
  plot(c(0,3*dist),c(0,3*dist),axes=F,typ='n')
  for (i in wbackground) 
    myBox(i)
  myC(2)
  myT(8)
}

the=function(){
  par(mar=c(0,0,0,0))
  plot(c(0,3*dist),c(0,3*dist),axes=F,typ='n')
  for (i in wbackground) 
    myBox(i)
  myT(2)
  myE(8)
}

myArray=function(surround){
  par(mar=c(0,0,0,0))
  plot(c(0,3*dist),c(0,3*dist),axes=F,typ='n')
  for (i in background) 
    myLet(surround/2,i)
}

myArray2=function(left,midleft,mid,midright,right){
  par(mar=c(0,0,0,0))
  plot(c(0,5*dist),c(0,dist),axes=F,typ='n')
  myLet(left/2,1)
  myLet(midleft/2,4)
  myLet(mid/2,7)
  myLet(midright/2,10)
  myLet(right/2,13)
}

myArray2(1,(14/20),(12/20),(10/20),0)
myArray(0)
myArray(1)
cat()
the()
```

## Procedure
Participants were presented with the stimuli and asked to judge whether the center letter was more similar to an "A" or an "H" by pressing the correpsonding keys on a standard keyboard.  Participants were explicitly instructed to ignore the background context and base their responses on the central target alone.

An experimental trial proceeded as follows: The screen was blank during a 1.5 sec foreperiod.  We warned participants that a target was about to appear as follows.  Two brief tones were presented 500 ms and 250 ms before the stimulus.  These tones allowed participants to precisely time the stimulus.  Next, the stimulus was presented for 100 ms, and thereafter, was replaced by a blank screen.  This blank screen remained until participant pressed either the "A" or "H" key to indicate their judgment about the target.  The response marked the end of the current trial and the beginning of the next one.  Responses and the time taken to respond was recorded.  A block consisted of 96 trials and the experimental session consisted of 10 blocks for a total of 960 trials.  Participants were encouraged to take breaks between blocks.  No feedback was given about participant responses during the course of the experimental session.  

All experimenal sessions were conducted on MacMini computers running the operating system MacOSX 10.6.2 with Octave version 3.2.3.  This experimental procedure was approved by the Institutional Review Board at the University of Missouri.

# Results

```{r datagrab, cache=TRUE}
##############
#DATA SETUP
##############
mysub=c(1:13,15,17:44,46:51,53:72,74:94,96:97,99:101)
mySubLab=sprintf("%03d",mysub)

fileroot="https://raw.githubusercontent.com/PerceptionCognitionLab/data1/master/ctxIndDif/flankerMorph4/flankerMorph4.dat."
filename=paste(fileroot,mySubLab,sep='')

inputDat=read.table(url(filename[1]))
for(i in 2:length(mysub)){
  inputDatPiece=read.table(url(filename[i]))
  inputDat=rbind(inputDat,inputDatPiece)
}

dat=inputDat
header <- c("sub", "trial", "block", "trialpblock", "file", "frame", "pixels", "response", "choice", "rt", "toofast", "toofastN")
colnames(dat) <- header

#############
#DATA SETUP
#############
dat.clean <- subset(dat,rt <2 & toofast == 0 & pixels != 1 & pixels != 5)
```

Data were cleaned by discarding responses with latencies less than 200 ms and greater than 2 sec. These discards comprised about 1\% of the total.  Additionally, the first twenty trials of the session were considered practice and excluded.  These criteria were chosen before data collection.

Figure \@ref(fig:avcfigures)A shows the proportion of "H" responses as a function of target and context.  As expected, the curves start low for *A* and *A*-like stimuli and increase as the targets become more *H*-like.  Solid and dashed lines denote letter and word contexts, respectively.  Filled and open points denote contexts that promote a "A" responses (*H*-letter and *C_T*-word contexts) and "H" responses (*A*-letter and *T_E*-word contexts), respectively.  A contrast effect may be seen for letter contexts.  Here, the *A*-letter context promoted higher proportions of "H" responses than the *H*-letter context did.  The opposed effect---assimilation---may be seen for the word contexts.  The context *T_E* promoted higher proportions of "H" responses than the *C_T* context did.

```{r avcfigures, fig.width=8,fig.height=4.5,fig.cap = "Empirical Results. **A.** Response proportions for all four contexts.  Solid and dashed lines indicate letter and word contexts, respectively.  Filled and open points indicate A-promoting and H-promoting contexts, respectively. **B.** Individual effects in the letter contexts. The negative direction denotes a robust contrast effect. **C.** Individual effects in the word contexts. The positive direction denotes a robust assimilation effect.", cache=TRUE}
library(graphics)
par(mfrow=c(1,3))
##############
#DATA SETUP
##############
dat.clean <- subset(dat, rt <2 & toofast == 0)

means <- tapply(dat.clean$response, list(dat.clean$frame, dat.clean$pixels, dat.clean$sub), mean,na.rm=T)

I <- length(unique(dat.clean$sub))
J <- length(unique(dat.clean$frame))

meanmeans.A = rowMeans(means[1,,])
meanmeans.CAT = rowMeans(means[2,,])
meanmeans.H = rowMeans(means[3,,])
meanmeans.THE = rowMeans(means[4,,])

###########
# Plot 1
###########
plot(meanmeans.A, type = 'b', col = "lightsteelblue4", main= "A.", ylab = "Proportion of 'H' Responses", xlab = "Target", ylim = c(-.05,1.05),lwd = 2,pch=1,axes=F)
axis(2)
axis(1,at=1:5,label=c("A",2:4,"H"))

lines(meanmeans.H, col = "slateblue4", type = 'b', pch=19, lwd = 2)
lines(meanmeans.CAT,col = "lightsteelblue4", type = 'b', pch=19, lwd = 2, lty=2)
lines(meanmeans.THE, col = "slateblue4", type = 'b', pch=1,lwd = 2, lty=2)
par(xpd=T)
legend(1,1.15,legend=c("A-letter","H-letter","C_T-word","T_E-word"),lty=c(1,1,2,2),lwd=c(1,1,1,1),pch=c(1,19,19,1),col=c("lightsteelblue4","slateblue4","lightsteelblue4","slateblue4"),bg="ghostwhite",title="Contexts")
par(xpd=F)

###########
# Plot 2
###########
plot((means[3,,1]-means[1,,1]), type = 'l', col=hsv(.6,1,1,.2), main= "B.", ylab = "Difference", xlab = "Target",ylim = c(-.8,.8),axes=F)
mtext("Letter Context", line = -1, padj = 0, side = 3, las = 1)
x=as.vector(round(seq(-.6,.6,.2),digits=1))
par(las=1)
axis(2,at=seq(-.8,.8,.2),labels=c("Favors A",x,"Favors H"))
par(las=0)
axis(1,at=1:5,label=c("A",2:4,"H"))
for(i in 2:I){
  lines((means[3,,i]-means[1,,i])
        , col=hsv(.6,1,1,.2))
}
meandiff <- rowMeans((means[3,,]-means[1,,]))
lines(meandiff, col = "darkred" , lwd = 3)
points(meandiff, col = "darkred", pch=19)
abline(h=0,lwd=3)
# legend(3,-.7,legend=c("H-A"),bg="ghostwhite",xjust=.5)
# rect(2.5,-.85,3.5,-.75,border="black",col="ghostwhite")
# mtext("H-A",side=1,line=-1.5,cex=.75)
maxcontrast=-1*min(meandiff)

###########
# Plot 3
###########
plot((means[4,,1]-means[2,,1]), type = 'l', col=hsv(.6,1,1,.2), main= "C.", ylab = "Difference", xlab = "Target",ylim = c(-.8,.8),axes=F)
mtext("Word Context", line = -1, padj = 0, side = 3, las = 1)
x=as.vector(round(seq(-.6,.6,.2),digits=1))
par(las=1)
axis(2,at=seq(-.8,.8,.2),labels=c("Favors A",x,"Favors H"))
par(las=0)
axis(1,at=1:5,label=c("A",2:4,"H"))
for(i in 2:I){
  lines((means[4,,i]-means[2,,i])
        , col=hsv(.6,1,1,.2))
}
meandiff <- rowMeans((means[4,,]-means[2,,]))
lines(meandiff, col = "darkred" , lwd = 3)
points(meandiff, col = "darkred", pch=19)
abline(h=0,lwd=3)
#legend(3,-.7,legend=c("T_E-C_T"),bg="ghostwhite",xjust=.5)
# rect(2,-.85,4,-.75,border="black",col="ghostwhite")
# mtext("T_E-C_T",side=1,line=-1.5,cex=.75)
maxassimilation=max(meandiff)
```

Figure \@ref(fig:avcfigures)A shows effects averaged across individuals.  Effects for each individual are shown in Panels B and C.   To observe individual effects in the letter-context condition, we subtracted the proportion of "H" responses for the *A*-letter context from that for the *H*-letter context.  In this graph, positive values indicate an assimilation effect; zero indicates no effect of context direction; negative values indicate a contrast effect.  The following three points are noted:  1) The contrast effects of letter contexts are robust across individuals.  2) The size of these effects is much larger than usual.  The differences in proportions average as much as `r round(maxcontrast,digits=2)`, which dwarfs the size of differences in most experiments.  3) The degree of individual variability is also quite large.  This degree provides increased resolution in the following correlational analysis.  Figure \@ref(fig:avcfigures)C shows the same plot for the word context; it is formed by subtracting the proportion of H responses in the *C_T*-word context from that in the *T_E*-word context.  The story about individuals is largely the same: 1) seemingly every individual shows an assimilation effect, 2) the effect is large, averaging as much as `r round(maxassimilation,digits=2)`, and 3) there is a suitable range of variation.

```{r chain, cache=TRUE, warning=FALSE}
#############
#LIBRARIES
#############
library("RCurl")
library("msm")
library("MASS")
library("rvest")
library("MCMCpack")
library("mvtnorm")

##############
#IMPORTANT VALUES
##############
I = length(unique(dat.clean$sub)) # number of participants
J = length(unique(dat.clean$frame))/2 # number of frame types
K = length(unique(dat.clean$frame))/2 #number of frame congruency
L = length(unique(dat.clean$pixels)) #number of morphs
n = 60 #number of replications

##############
#DATA SETUP, DEFINITION:  ACCURACY RESPONSES
##############
y=dat.clean$response
N=length(y)

sub=as.factor(dat.clean$sub)
levels(sub)=1:I
sub = as.numeric(sub)
stim=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="THE",1,0)
frame=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="H",-.5,.5)
morph=dat.clean$pixels-1

##############
#CHAIN SETUP
##############
#REPITIONS
##############
M=1000

##############
#VALUES TO ESTIMATE
##############
gammas = matrix(nrow=M,ncol=L*I)
alpha = matrix(nrow=M,ncol=I)
beta = matrix(nrow=M,ncol=I)

theta = cbind(gammas,alpha,beta)
delta = 1:M

mu = matrix(nrow=M,ncol=L+2)
omega=array(dim=c(M,2,2))

##############
#PRIOR SETTINGS
##############
munull = rep(0,L*I+I+I)
s2null = rep(1,L*I+I+I)
omegaI = matrix(nrow=2,c(.05,0,0,.05)) #note omegaI stands for omega inverse; it is inversed because in precision

#for delta
a=.5
b=.01


##############
#STARTING VALUES
##############
theta[1,] = rep(0,L*I+I+I)
mu[1,] = rep(0,L+2)
#delta taken care of
startVal=.15^2
omega[1,,]=solve(matrix(nrow=2,c(startVal,.5*startVal,.5*startVal,startVal)))

# Creating B matrix for estimating theta's variance, note B is in precision, variance's inverse (saves time inverting later)
blockVec=function(i){
  (I*L+(i-1)*2+1):(I*L+2*i)
}

B=matrix(0,nrow=L*I+I+I,ncol=L*I+I+I)

##############
#EXTRA CALCULATIONS
##############
#Creating Matrices
X.gammas = matrix(0,nrow=N,ncol=L*I)
for (n in 1:N){
  X.gammas[n,(sub[n]-1)*L+morph[n]]=1
}
X.AB = matrix(0,nrow=N,ncol=2*I)
for (n in 1:N){
  X.AB[n,(sub[n]-1)*2+1]=stim[n]*frame[n]
  X.AB[n,(sub[n]-1)*2+2]=(1-stim[n])*frame[n]
}

X.add = cbind(X.gammas,X.AB)

mu.morph = rep(1:L,I)
X.mean = matrix(0,nrow=I*(L+2),ncol=L+2)
for(n in 1:length(mu.morph)){
  X.mean[n,mu.morph[n]]=1
}
for(n in (I*L+1):(I*(L+2))){
  X.mean[n,L+(n-I*L+1)%%2+1]=1 
}

#Logic Arguments
lower=upper=1:N
lower[y==1]=0
upper[y==1]=Inf
lower[y==0]=-Inf
upper[y==0]=0

#Other
alpha.idx=seq(I*L+1,I*(L+2),2)
beta.idx=seq(I*L+2,I*(L+2),2)

##############
#GIBBS SAMPLING CHAIN
##############
for(m in 2:M){
  w=rtnorm(N,X.add%*%theta[m-1,],1,lower,upper)
  
  #set B
  diag(B)[1:(I*L)]=1/delta[m-1]
  for (i in 1:I) {
    B[blockVec(i),blockVec(i)]=omega[m-1,,]
  }
  
  #set theta
  munull = X.mean%*%mu[m-1,] #prior
  
  V=solve(crossprod(X.add)+B)
  C=t(X.add)%*%(w)+B%*%munull
  theta[m,]=mvrnorm(1,V%*%C,V)
  
  #set mu
  V.mu = solve(t(X.mean)%*%B%*%X.mean+diag(L+2))
  C.mu = t(X.mean)%*%B%*%theta[m,] # plus something but it's zero right now in the prior on mu
  mu[m,] = mvrnorm(1,V.mu%*%C.mu,V.mu)
  
  #set delta
  delta.shape = a +(I*L)/2
  delta.scale = b + .5*sum((theta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta[m] = rinvgamma(1,shape=delta.shape,scale=delta.scale)
  
  #set omega
  #from Wikipedia and Jeff
  effects=cbind(theta[m,alpha.idx],theta[m,beta.idx])
  muEffects=c(mu[m,L+1],mu[m,L+2])
  diffs=t(t(effects)-muEffects)
  omega[m,,]=rwish(3+I,solve(omegaI+t(diffs)%*%diffs))
}

pm=apply(theta,2,mean)
pvar=apply(theta,2,var)
pcor=cor(pm[alpha.idx],pm[beta.idx])
```

```{r chainvariables, cache=TRUE,warning=FALSE}
pma=pm[alpha.idx]
pmb=pm[beta.idx]
pva=pvar[alpha.idx]
pvb =pvar[beta.idx]
```

```{r indchainminus, cache=TRUE, warning=FALSE}
#############
#DATA SETUP
#############
dat.clean <- subset(dat,rt <2 & toofast == 0 & pixels != 1 & pixels != 5 & sub!=75)

##############
#IMPORTANT VALUES
##############
I = length(unique(dat.clean$sub)) # number of participants
J = length(unique(dat.clean$frame))/2 # number of frame types
K = length(unique(dat.clean$frame))/2 #number of frame congruency
L = length(unique(dat.clean$pixels)) #number of morphs
n = 60 #number of replications

##############
#DATA SETUP, DEFINITION:  ACCURACY RESPONSES
##############
y=dat.clean$response
N=length(y)

sub=as.factor(dat.clean$sub)
levels(sub)=1:I
sub = as.numeric(sub)
stim=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="THE",1,0)
frame=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="H",-.5,.5)
morph=dat.clean$pixels-1

##############
#CHAIN SETUP
##############
#REPITIONS
##############
M=1000

##############
#VALUES TO ESTIMATE
##############
gammas = matrix(nrow=M,ncol=L*I)
alpha = matrix(nrow=M,ncol=I)
beta = matrix(nrow=M,ncol=I)

indminustheta = cbind(gammas,alpha,beta)

delta.gamma = 1:M
delta.alpha = 1:M
delta.beta = 1:M

mu.gamma = matrix(nrow=M,ncol=L)
mu.alpha = matrix(nrow=M,ncol=1)
mu.beta = matrix(nrow=M,ncol=1)

mu=cbind(mu.gamma,mu.alpha,mu.beta)

##############
#PRIOR SETTINGS
##############
munull = rep(0,L*I+I+I)
s2null = rep(1,L*I+I+I)

#for deltas
a=.5
b=.01

r=.6
s=.02

m=.4
n=.03

##############
#STARTING VALUES
##############
indminustheta[1,] = rep(0,L*I+I+I)
mu[1,] = rep(0,L+2)
#delta taken care of

# Creating B matrix for estimating theta's variance, note B is in precision, variance's inverse (saves time inverting later)
B=matrix(0,nrow=L*I+I+I,ncol=L*I+I+I)

##############
#EXTRA CALCULATIONS
##############
#Creating Matrices
X.gammas = matrix(0,nrow=N,ncol=L*I)
for (n in 1:N){
  X.gammas[n,(sub[n]-1)*L+morph[n]]=1
}
X.alpha = matrix(0,nrow=N,ncol=I)
for (n in 1:N){
  X.alpha[n,sub[n]]=stim[n]*frame[n]
}
X.beta = matrix(0,nrow=N,ncol=I)
for (n in 1:N){
  X.beta[n,sub[n]]=(1-stim[n])*frame[n]
}

X.add = cbind(X.gammas,X.alpha,X.beta)

mu.morph = rep(1:L,I)
X.mean = matrix(0,nrow=I*(L+2),ncol=L+2)
for(n in 1:length(mu.morph)){
  X.mean[n,mu.morph[n]]=1
}
for(n in (I*L+1):(I*(L+2))){
  X.mean[n,L+(n-I*L+1)%%2+1]=1 
}

#Logic Arguments
lower=upper=1:N
lower[y==1]=0
upper[y==1]=Inf
lower[y==0]=-Inf
upper[y==0]=0

#Other
alpha.idx=seq(I*L+1,I*(L+1))
beta.idx=seq(I*(L+1)+1,I*(L+2))

##############
#GIBBS SAMPLING CHAIN
##############
for(m in 2:M){
  w=rtnorm(N,X.add%*%indminustheta[m-1,],1,lower,upper)
  
  #set B
  diag(B)[1:(I*L)]=1/delta.gamma[m-1]
  diag(B)[(I*L):(I*(L+1))]=1/delta.alpha[m-1]
  diag(B)[(I*(L+1)):(I*(L+2))]=1/delta.beta[m-1]
  
  #set theta
  munull = X.mean%*%mu[m-1,] #prior
  
  V=solve(crossprod(X.add)+B)
  C=t(X.add)%*%(w)+B%*%munull
  indminustheta[m,]=mvrnorm(1,V%*%C,V)
  
  #set mu
  V.mu = solve(t(X.mean)%*%B%*%X.mean+diag(L+2))
  C.mu = t(X.mean)%*%B%*%indminustheta[m,] # plus something but it's zero right now in the prior on mu
  mu[m,] = mvrnorm(1,V.mu%*%C.mu,V.mu)
  
  #set delta.gamma
  delta.gamma.shape = a +(I*L)/2
  delta.gamma.scale = b + .5*sum((indminustheta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta.gamma[m] = rinvgamma(1,shape=delta.gamma.shape,scale=delta.gamma.scale)
  
  #set delta.alpha
  delta.alpha.shape = r +(I*L)/2
  delta.alpha.scale = s + .5*sum((indminustheta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta.alpha[m] = rinvgamma(1,shape=delta.alpha.shape,scale=delta.alpha.scale)
  
  #set delta.beta
  delta.beta.shape = m +(I*L)/2
  delta.beta.scale = n + .5*sum((indminustheta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta.beta[m] = rinvgamma(1,shape=delta.beta.shape,scale=delta.beta.scale)
}
alpha.idx=seq(I*L+1,I*(L+1))
beta.idx=seq(I*(L+1)+1,I*(L+2))
indminuscor=1:M
for (r in 2:M) indminuscor[r]=cor(indminustheta[r,alpha.idx],indminustheta[r,beta.idx])
```

```{r chainminus, cache=TRUE, warning=FALSE}
#############
#DATA SETUP
#############
dat.clean <- subset(dat,rt <2 & toofast == 0 & pixels != 1 & pixels != 5 & sub!=75)

##############
#IMPORTANT VALUES
##############
I = length(unique(dat.clean$sub)) # number of participants
J = length(unique(dat.clean$frame))/2 # number of frame types
K = length(unique(dat.clean$frame))/2 #number of frame congruency
L = length(unique(dat.clean$pixels)) #number of morphs
n = 60 #number of replications

##############
#DATA SETUP, DEFINITION:  ACCURACY RESPONSES
##############
y=dat.clean$response
N=length(y)

sub=as.factor(dat.clean$sub)
levels(sub)=1:I
sub = as.numeric(sub)
stim=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="THE",1,0)
frame=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="H",-.5,.5)
morph=dat.clean$pixels-1

##############
#CHAIN SETUP
##############
#REPITIONS
##############
M=1000

##############
#VALUES TO ESTIMATE
##############
gammas = matrix(nrow=M,ncol=L*I)
alpha = matrix(nrow=M,ncol=I)
beta = matrix(nrow=M,ncol=I)

minustheta = cbind(gammas,alpha,beta)
delta = 1:M

mu = matrix(nrow=M,ncol=L+2)
omega=array(dim=c(M,2,2))

##############
#PRIOR SETTINGS
##############
munull = rep(0,L*I+I+I)
s2null = rep(1,L*I+I+I)
omegaI = matrix(nrow=2,c(.05,0,0,.05)) #note omegaI stands for omega inverse; it is inversed because in precision

#for delta
a=.5
b=.01


##############
#STARTING VALUES
##############
minustheta[1,] = rep(0,L*I+I+I)
mu[1,] = rep(0,L+2)
#delta taken care of
startVal=.15^2
omega[1,,]=solve(matrix(nrow=2,c(startVal,.5*startVal,.5*startVal,startVal)))

# Creating B matrix for estimating minustheta's variance, note B is in precision, variance's inverse (saves time inverting later)
blockVec=function(i){
  (I*L+(i-1)*2+1):(I*L+2*i)
}

B=matrix(0,nrow=L*I+I+I,ncol=L*I+I+I)

##############
#EXTRA CALCULATIONS
##############
#Creating Matrices
X.gammas = matrix(0,nrow=N,ncol=L*I)
for (n in 1:N){
  X.gammas[n,(sub[n]-1)*L+morph[n]]=1
}
X.AB = matrix(0,nrow=N,ncol=2*I)
for (n in 1:N){
  X.AB[n,(sub[n]-1)*2+1]=stim[n]*frame[n]
  X.AB[n,(sub[n]-1)*2+2]=(1-stim[n])*frame[n]
}

X.add = cbind(X.gammas,X.AB)

mu.morph = rep(1:L,I)
X.mean = matrix(0,nrow=I*(L+2),ncol=L+2)
for(n in 1:length(mu.morph)){
  X.mean[n,mu.morph[n]]=1
}
for(n in (I*L+1):(I*(L+2))){
  X.mean[n,L+(n-I*L+1)%%2+1]=1 
}

#Logic Arguments
lower=upper=1:N
lower[y==1]=0
upper[y==1]=Inf
lower[y==0]=-Inf
upper[y==0]=0

#Other
alpha.idx=seq(I*L+1,I*(L+2),2)
beta.idx=seq(I*L+2,I*(L+2),2)

##############
#GIBBS SAMPLING CHAIN
##############
for(m in 2:M){
  w=rtnorm(N,X.add%*%minustheta[m-1,],1,lower,upper)
  
  #set B
  diag(B)[1:(I*L)]=1/delta[m-1]
  for (i in 1:I) {
    B[blockVec(i),blockVec(i)]=omega[m-1,,]
  }
  
  #set minustheta
  munull = X.mean%*%mu[m-1,] #prior
  
  V=solve(crossprod(X.add)+B)
  C=t(X.add)%*%(w)+B%*%munull
  minustheta[m,]=mvrnorm(1,V%*%C,V)
  
  #set mu
  V.mu = solve(t(X.mean)%*%B%*%X.mean+diag(L+2))
  C.mu = t(X.mean)%*%B%*%minustheta[m,] # plus something but it's zero right now in the prior on mu
  mu[m,] = mvrnorm(1,V.mu%*%C.mu,V.mu)
  
  #set delta
  delta.shape = a +(I*L)/2
  delta.scale = b + .5*sum((minustheta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta[m] = rinvgamma(1,shape=delta.shape,scale=delta.scale)
  
  #set omega
  #from Wikipedia and Jeff
  effects=cbind(minustheta[m,alpha.idx],minustheta[m,beta.idx])
  muEffects=c(mu[m,L+1],mu[m,L+2])
  diffs=t(t(effects)-muEffects)
  omega[m,,]=rwish(3+I,solve(omegaI+t(diffs)%*%diffs))
}
```

To assess the correlation among individual assimilation and contrast effects, we developed a Bayesian hierarchical mixed model with a probit link.  The benefits of the modeling approach are two-fold:  First, it provides a principled means of combining data across the different targets.  Second, and more importantly, the hierarchical structure provides a form of regularization used to avoid overstating the range of individual variation [@Efron:Morris:1977;@Lehmann:Casella:1998;@Davis:etal:2017].  The model and corresponding analysis are described in an online supplement at https://github.com/PerceptionAndCognitionLab/ctx-flanker/tree/public/papers/current.  The main outputs are individual estimates of assimilation and contrast effects and a posterior distribution of the correlation.  The individual estimates are shown in the scatter plot in Figure \@ref(fig:modelfiguresminus)A; more positive values indicate a stronger assimilation and stronger contrast effects in the respective background contexts.

```{r modelfiguresminus, fig.width = 6, fig.height = 4, fig.cap = "Model Results.  **A.** Each participant's estimated assimilation effect against their estimated contrast effect.  An ellipse denotes the standard deviations of the estimate and the blue regression line is the line of best fit.  **B.** The posterior distribution of the population correlation, $\\rho$, between assimilation and contrast.  The solid line denotes the prior distribution.",cache=TRUE}
#layout(matrix(c(1,1,1,4,4,4,1,1,1,4,4,4,1,1,1,4,4,4,2,2,2,3,3,3,2,2,2,3,3,3),nrow=5,byrow=T))
#fig.width = 6, fig.height = 7.2
par(mfrow=c(1,2))
###########
# Plot 1
###########
library(ellipse)
library(graphics)
# par(pty='s')
myColor=hsv((1:I)/I,1,.7,1)
plot(pma,pmb,col=myColor,pch=20,ylim=c(-.5,2.5),xlim=c(-.5,2.5), main= "A.", ylab = "Contrast Effect", xlab = "Assimilation Effect",frame.plot = F)
myColor=hsv((1:I)/I,1,.7,.35)
for (i in 1:I){
  mat=matrix(c(pva[i],pcor*sqrt(pva[i])*sqrt(pvb[i]),pcor*sqrt(pva[i])*sqrt(pvb[i]),pvb[i]),nrow=2)
  lines(ellipse(mat,centre = c(pma[i],pmb[i]),level=pnorm(1)-pnorm(-1)),col=myColor[i])
}
abline(0,1)
abline(lm(pmb~pma),col="slateblue",lwd=2)
abline(v=0,col="gray",lty=3)
abline(h=0,col="gray",lty=3)
arrows(2,0,1.6,-.05,length=.075, angle=30,code=2,lwd=2)

# ###########
# # Plot 2
# ###########
# indminus.hist=hist(indminuscor[2:M],prob=T,xlim=c(-1,1),col='red',breaks = seq(-1.025,1.025,.05),main = " C.", xlab = "Correlation")
# 
# f=function (r,n){
#   num=(1-r^2)^((n-4)/2)
#   den=beta(.5,(n-2)/2)
#   p.r = (num)/(den)
#   return(p.r)
# }
# lines(x=seq(-1,1,.01),y=f(seq(-1,1,.01),I),typ="l")
# abline(v=0,col="gray",lty=3)
# 
# indminusfactor =f(0,I)/indminus.hist$density[21]
# 
# meanindminuscor=mean(indminuscor)
# 
# ###########
# # Plot 3
# ###########
# alpha.idx=seq(I*L+1,I*(L+2),2)
# beta.idx=seq(I*L+2,I*(L+2),2)
# minuscor=1:M
# for (r in 2:M) minuscor[r]=cor(minustheta[r,alpha.idx],minustheta[r,beta.idx])
# 
# minuscor.hist=hist(minuscor[2:M],prob=T,xlim=c(-1,1),col='red',breaks = seq(-1.025,1.025,.05), main = "D.", xlab = "Correlation")
# abline(h=.5)
# abline(v=0,col="gray",lty=3)
# 
# minuscorfactor =.5/minuscor.hist$density[21]
# 
# meanminuscor = mean(minuscor)

###########
# Plot 4
###########
myCor=function(precision)
{
  cov=solve(precision)
  return(cov[1,2]/(sqrt(cov[1,1]*cov[2,2])))
}

rho=1:M
for (r in 1:M)
  rho[r]=myCor(omega[r,,])

#random effects
rho.hist=hist(rho,prob=T,col='lightblue',xlim=c(-1,1),ylim=c(0,4),breaks = seq(-1.025,1.025,.05),main = "B.", xlab = "Population Correlation")
abline(h=.5)
abline(v=0,col="gray",lty=3)
meanrho=mean(rho)
rhofactor =.5/rho.hist$density[21]
```

```{r even, cache=TRUE, warning=FALSE}
#############
#DATA SETUP
#############
dat.clean <- subset(dat,rt <2 & toofast == 0 & pixels != 1 & pixels != 5)
FullN = nrow(dat.clean)

Even = seq(2,FullN,2)
Odd = seq(1,FullN,2)

dat.clean = dat.clean[Even,]

##############
#IMPORTANT VALUES
##############
I = length(unique(dat.clean$sub)) # number of participants
J = length(unique(dat.clean$frame))/2 # number of frame types
K = length(unique(dat.clean$frame))/2 #number of frame congruency
L = length(unique(dat.clean$pixels)) #number of morphs
n = 60 #number of replications

##############
#DATA SETUP, DEFINITION:  ACCURACY RESPONSES
##############
y=dat.clean$response
N=length(y)

sub=as.factor(dat.clean$sub)
levels(sub)=1:I
sub = as.numeric(sub)
stim=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="THE",1,0)
frame=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="H",-.5,.5)
morph=dat.clean$pixels-1

##############
#CHAIN SETUP
##############
#REPITIONS
##############
M=1000

##############
#VALUES TO ESTIMATE
##############
gammas = matrix(nrow=M,ncol=L*I)
alpha = matrix(nrow=M,ncol=I)
beta = matrix(nrow=M,ncol=I)

theta = cbind(gammas,alpha,beta)
delta = 1:M

mu = matrix(nrow=M,ncol=L+2)
omega=array(dim=c(M,2,2))

##############
#PRIOR SETTINGS
##############
munull = rep(0,L*I+I+I)
s2null = rep(1,L*I+I+I)
omegaI = matrix(nrow=2,c(.05,0,0,.05)) #note omegaI stands for omega inverse; it is inversed because in precision

#for delta
a=.5
b=.01


##############
#STARTING VALUES
##############
theta[1,] = rep(0,L*I+I+I)
mu[1,] = rep(0,L+2)
#delta taken care of
startVal=.15^2
omega[1,,]=solve(matrix(nrow=2,c(startVal,.5*startVal,.5*startVal,startVal)))

# Creating B matrix for estimating theta's variance, note B is in precision, variance's inverse (saves time inverting later)
blockVec=function(i){
  (I*L+(i-1)*2+1):(I*L+2*i)
}

B=matrix(0,nrow=L*I+I+I,ncol=L*I+I+I)

##############
#EXTRA CALCULATIONS
##############
#Creating Matrices
X.gammas = matrix(0,nrow=N,ncol=L*I)
for (n in 1:N){
  X.gammas[n,(sub[n]-1)*L+morph[n]]=1
}
X.AB = matrix(0,nrow=N,ncol=2*I)
for (n in 1:N){
  X.AB[n,(sub[n]-1)*2+1]=stim[n]*frame[n]
  X.AB[n,(sub[n]-1)*2+2]=(1-stim[n])*frame[n]
}

X.add = cbind(X.gammas,X.AB)

mu.morph = rep(1:L,I)
X.mean = matrix(0,nrow=I*(L+2),ncol=L+2)
for(n in 1:length(mu.morph)){
  X.mean[n,mu.morph[n]]=1
}
for(n in (I*L+1):(I*(L+2))){
  X.mean[n,L+(n-I*L+1)%%2+1]=1 
}

#Logic Arguments
lower=upper=1:N
lower[y==1]=0
upper[y==1]=Inf
lower[y==0]=-Inf
upper[y==0]=0

#Other
alpha.idx=seq(I*L+1,I*(L+2),2)
beta.idx=seq(I*L+2,I*(L+2),2)

##############
#GIBBS SAMPLING CHAIN
##############
for(m in 2:M){
  w=rtnorm(N,X.add%*%theta[m-1,],1,lower,upper)
  
  #set B
  diag(B)[1:(I*L)]=1/delta[m-1]
  for (i in 1:I) {
    B[blockVec(i),blockVec(i)]=omega[m-1,,]
  }
  
  #set theta
  munull = X.mean%*%mu[m-1,] #prior
  
  V=solve(crossprod(X.add)+B)
  C=t(X.add)%*%(w)+B%*%munull
  theta[m,]=mvrnorm(1,V%*%C,V)
  
  #set mu
  V.mu = solve(t(X.mean)%*%B%*%X.mean+diag(L+2))
  C.mu = t(X.mean)%*%B%*%theta[m,] # plus something but it's zero right now in the prior on mu
  mu[m,] = mvrnorm(1,V.mu%*%C.mu,V.mu)
  
  #set delta
  delta.shape = a +(I*L)/2
  delta.scale = b + .5*sum((theta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta[m] = rinvgamma(1,shape=delta.shape,scale=delta.scale)
  
  #set omega
  #from Wikipedia and Jeff
  effects=cbind(theta[m,alpha.idx],theta[m,beta.idx])
  muEffects=c(mu[m,L+1],mu[m,L+2])
  diffs=t(t(effects)-muEffects)
  omega[m,,]=rwish(3+I,solve(omegaI+t(diffs)%*%diffs))
}

##############
#POSTERIOR RESULTS
##############
pm=apply(theta,2,mean)

#Estimated values
post.gammas=pm[1:(L*I)]
post.alpha=pm[alpha.idx]
post.beta=pm[beta.idx]

even = cbind(post.alpha,post.beta)
```

```{r odd, cache=TRUE, warning=FALSE}
##############
#DATA SETUP
##############
dat.clean <- subset(dat,rt <2 & toofast == 0 & pixels != 1 & pixels != 5)
FullN = nrow(dat.clean)

Even = seq(2,FullN,2)
Odd = seq(1,FullN,2)

dat.clean = dat.clean[Odd,]

##############
#IMPORTANT VALUES
##############
I = length(unique(dat.clean$sub)) # number of participants
J = length(unique(dat.clean$frame))/2 # number of frame types
K = length(unique(dat.clean$frame))/2 #number of frame congruency
L = length(unique(dat.clean$pixels)) #number of morphs
n = 60 #number of replications

##############
#DATA SETUP, DEFINITION:  ACCURACY RESPONSES
##############
y=dat.clean$response
N=length(y)

sub=as.factor(dat.clean$sub)
levels(sub)=1:I
sub = as.numeric(sub)
stim=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="THE",1,0)
frame=ifelse(dat.clean$frame=="CAT"|dat.clean$frame=="H",-.5,.5)
morph=dat.clean$pixels-1

##############
#CHAIN SETUP
##############
#REPITIONS
##############
M=1000

##############
#VALUES TO ESTIMATE
##############
gammas = matrix(nrow=M,ncol=L*I)
alpha = matrix(nrow=M,ncol=I)
beta = matrix(nrow=M,ncol=I)

theta = cbind(gammas,alpha,beta)
delta = 1:M

mu = matrix(nrow=M,ncol=L+2)
omega=array(dim=c(M,2,2))

##############
#PRIOR SETTINGS
##############
munull = rep(0,L*I+I+I)
s2null = rep(1,L*I+I+I)
omegaI = matrix(nrow=2,c(.05,0,0,.05)) #note omegaI stands for omega inverse; it is inversed because in precision

#for delta
a=.5
b=.01


##############
#STARTING VALUES
##############
theta[1,] = rep(0,L*I+I+I)
mu[1,] = rep(0,L+2)
#delta taken care of
startVal=.15^2
omega[1,,]=solve(matrix(nrow=2,c(startVal,.5*startVal,.5*startVal,startVal)))

# Creating B matrix for estimating theta's variance, note B is in precision, variance's inverse (saves time inverting later)
blockVec=function(i){
  (I*L+(i-1)*2+1):(I*L+2*i)
}

B=matrix(0,nrow=L*I+I+I,ncol=L*I+I+I)

##############
#EXTRA CALCULATIONS
##############
#Creating Matrices
X.gammas = matrix(0,nrow=N,ncol=L*I)
for (n in 1:N){
  X.gammas[n,(sub[n]-1)*L+morph[n]]=1
}
X.AB = matrix(0,nrow=N,ncol=2*I)
for (n in 1:N){
  X.AB[n,(sub[n]-1)*2+1]=stim[n]*frame[n]
  X.AB[n,(sub[n]-1)*2+2]=(1-stim[n])*frame[n]
}

X.add = cbind(X.gammas,X.AB)

mu.morph = rep(1:L,I)
X.mean = matrix(0,nrow=I*(L+2),ncol=L+2)
for(n in 1:length(mu.morph)){
  X.mean[n,mu.morph[n]]=1
}
for(n in (I*L+1):(I*(L+2))){
  X.mean[n,L+(n-I*L+1)%%2+1]=1 
}

#Logic Arguments
lower=upper=1:N
lower[y==1]=0
upper[y==1]=Inf
lower[y==0]=-Inf
upper[y==0]=0

#Other
alpha.idx=seq(I*L+1,I*(L+2),2)
beta.idx=seq(I*L+2,I*(L+2),2)

##############
#GIBBS SAMPLING CHAIN
##############
for(m in 2:M){
  w=rtnorm(N,X.add%*%theta[m-1,],1,lower,upper)
  
  #set B
  diag(B)[1:(I*L)]=1/delta[m-1]
  for (i in 1:I) {
    B[blockVec(i),blockVec(i)]=omega[m-1,,]
  }
  
  #set theta
  munull = X.mean%*%mu[m-1,] #prior
  
  V=solve(crossprod(X.add)+B)
  C=t(X.add)%*%(w)+B%*%munull
  theta[m,]=mvrnorm(1,V%*%C,V)
  
  #set mu
  V.mu = solve(t(X.mean)%*%B%*%X.mean+diag(L+2))
  C.mu = t(X.mean)%*%B%*%theta[m,] # plus something but it's zero right now in the prior on mu
  mu[m,] = mvrnorm(1,V.mu%*%C.mu,V.mu)
  
  #set delta
  delta.shape = a +(I*L)/2
  delta.scale = b + .5*sum((theta[m,1:(I*L)]-(X.mean%*%mu[m,])[1:(I*L)])^2)
  delta[m] = rinvgamma(1,shape=delta.shape,scale=delta.scale)
  
  #set omega
  #from Wikipedia and Jeff
  effects=cbind(theta[m,alpha.idx],theta[m,beta.idx])
  muEffects=c(mu[m,L+1],mu[m,L+2])
  diffs=t(t(effects)-muEffects)
  omega[m,,]=rwish(3+I,solve(omegaI+t(diffs)%*%diffs))
}

##############
#POSTERIOR RESULTS
##############
pm=apply(theta,2,mean)

#Estimated values
post.gammas=pm[1:(L*I)]
post.alpha=pm[alpha.idx]
post.beta=pm[beta.idx]

odd = cbind(post.alpha,post.beta)
```

```{r reliability, cache=TRUE, warning=FALSE}
retest = cbind(even,odd)

alphacor = cor(retest[1:I,1],retest[1:I,3])
betacor = cor(retest[1:I,2],retest[1:I,4])

adjust=function(rho){
  (2*rho)/(1+rho)
}

adjalphacor = adjust(alphacor)
adjbetacor = adjust(betacor)
```

As can be seen in Figure \@ref(fig:modelfiguresminus)A, there is a fair degree of variation across individuals as well as an overall positive relationship.  An issue, however, is the presence of an outlying point, indicated with an arrow. This participant had the highest degree of assimilation and the lowest degree of contrast.  Performance here stands apart from that of the other participants; if the points is included in analysis, then it would have great leverage.  We decided in a *post-hoc* manner to exclude this participant, and the following analyses are based on this exclusion.   Our results therefore pertain to the vast majority of individuals in the main cluster.

Assimilation and contrast estimates show a positive relationship as seen by the OLS regression line in Figure \@ref(fig:modelfiguresminus)A.  OLS regression, however, is inappropriate as an inferential tool because the estimates are correlated through the hierarchical structure.  To perform inference, we plot the prior and posterior distributions of the population-level correlation coefficient.  The prior distribution here was chosen to be flat, placing equal plausibility on all values of the correlation coefficient.  The resulting posterior distribution is well localized for positive values away from zero.  The mean of this posterior distribution serves as a point estimate, and it is `r round(meanrho,digits=2)`.   One way of competitively assessing the null-correlation hypothesis vs. the alternative is to compute the change in plausibility at zero.  The plausibility was greatly reduced by a factor of `r round(rhofactor,digits=2)`.  This indicates the data are `r round(rhofactor,digits=2)` times more plausible under the alternative than the null, and this computation is the Savage-Dickey approach to Bayes factors [@Dickey:1971;@Gelfand:Smith:1990].

To place this correlation in context, it is helpful to consider the reliability of individual estimates.  We suspected high reliability because we had each participant perform a total of 960 trials, which is quite numerous.  We split each individual's data into odd and even trials, and reran the Bayesian probit regression analysis separately for the odd and even trials.  Each analysis provides an estimate of each individual's assimilation and contrast, and the assimilation estimates in the odd trials may be correlated with the assimilation in the even trials, and the same for the contrast estimates.  The correlation among individuals' assimilation estimates was `r round(alphacor,digits=2)`; the correlation among individuals' contrast estimates was `r round(betacor,digits=2)`.  These values when extrapolated to the full sample imply reliabilities of `r round(adjalphacor,digits=2)` and `r round(adjbetacor,digits=2)`, respectively.  Hence, much of the variation in the scatter is not due to trial-by-trial noise, but reflect true latent variation across individuals.

<!-- We used `r cite_r("r-references.bib")` for all our analyses. -->

# Discussion
Our goal here was to explore whether inhibition was mediate by a common or distinct mechanisms in assimilation and contrast contexts.  On one hand, inhibition is often conceptualized as a well integrated concept  [e.g., @Miyake:etal:2000].  On the other hand, assimilation is often thought of as a response-level cognitive process [@Eriksen:Eriksen:1974] whereas contrast is thought of as a low-level perceptual process [e.g, @Palmer:1999].  To explore this question, we assessed the correlation between individuals' ability to inhibit background assimilative information and their ability to inhibit background contrastive information.  Our approach relied on morph-letter targets.  Large assimilation effects were found when the surrounding, to-be-ignored information could potentially be a word, replicating the often observed top-down effect of word contexts on letter identification.  Contrast effects were found when the surrounding contexts were letters, replicating @Rouder:King:2003.  The key finding is a positive correlation across individuals.  Individuals who were better able to inhibit the contrastive effects of surrounding letter contexts  were better able to inhibit the assimilative effects of surrounding word contexts.

The findings are surprising inasmuch as contrast and assimilation effects are generally thought to have different loci.  We think the correlation is of interest in itself.  Perhaps the most parsimonious account is from @Lachter:etal:2004, who provide an updated version of Broadband's classic early-attention theory [@Broadbent:1958].  Here attention acts fairly early but is imperfect and some irrelevant information is processed.  When it is, the ensuing contrast and assimilation effects  result.  In this view, the common variation across these tasks indexes the individual's raw ability to control selective attention.  Linking correlations with processing must be done tentatively.  The next step forward in this line of research is exploring how the correlations across contrastive and assimilative tasks interact with manipulations designed to affect spatial attention.

#### Author's Contributions

J. N. Rouder and S. M. Rafferty developed the study concept.  All authors contributed to the study design.  Programming of the experimental desgin was done by H. K. Snyder.  Data collection was led by S. M. Rafferty.  H. K. Snyder performed the data analysis and interpretation with the aid of J. M. Haaf under the supervision of J. N. Rouder.  H. K. Snyder and J. N. Rouder drafted the manuscript; S. M. Rafferty and J. M. Haaf provided critical revisions.  All authors approved the final version of the manuscript for submission.

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
